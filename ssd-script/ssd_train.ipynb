{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ssd_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTqw2jRvYNqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib9ZfSNgvLdV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "#path of ssd directory\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/ssd.zip\", 'r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()\n",
        "!pip install filterpy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMTBe83NH4Hz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#download UA-Track dataset\n",
        "!wget http://detrac-db.rit.albany.edu/Data/DETRAC-train-data.zip\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCk249rUJn2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#extract dataset\n",
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"DETRAC-train-data.zip\")\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pldky8G5v_Du",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train script\n",
        "\n",
        "from ssd import *\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn.init as init\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import argparse\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "num_workers = 4\n",
        "lr = 1e-3\n",
        "momentum=0.9\n",
        "weight_decay= 5e-4\n",
        "gamma=0.1\n",
        "#path of train image directory\n",
        "imgs_path = './Insight-MVT_Annotation_Train/'\n",
        "#path of image annotation\n",
        "# image_path                                   box1                 box2                   box3....\n",
        "#./Insight-MVT_Annotation_Train/image1.jpg, x1_1,y1_1,x2_1,y2_1, x1_2,y1_2,x2_2,y2_2 .....\n",
        "labels_path = '/content/drive/My Drive/ssd_train.csv'\n",
        "\n",
        "input_size = 300\n",
        "num_classes = 2\n",
        "use_cuda = True\n",
        "epochs=60000\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "    cuda=True\n",
        "else:\n",
        "    torch.set_default_tensor_type('torch.FloatTensor')\n",
        "    cuda = False\n",
        "\n",
        "\n",
        "dataset = Transform(imgs_path,labels_path,transform=SSDAugmentation())\n",
        "\n",
        "dataset_len = len(dataset)\n",
        "print(\"dataset len\",dataset_len)\n",
        "\n",
        "#load data\n",
        "train_loader = DataLoader(dataset,batch_size,num_workers=0, collate_fn=detection_collate)\n",
        "data_loaders = {\"train\": train_loader, \"val\": train_loader}\n",
        "\n",
        "\n",
        "start_epoch=48000\n",
        "#path of pretrained file\n",
        "model_path = \"/content/drive/My Drive/checkpoints/ssd_ckpt_%d.pth\"% start_epoch\n",
        "optim_path = \"/content/drive/My Drive/checkpoints/optim_ckpt_%d.pth\"% start_epoch\n",
        "\n",
        "ssd_net = build_ssd('train', cfg['min_dim'], cfg['num_classes'])\n",
        "net = ssd_net\n",
        "ssd_net.load_state_dict(torch.load(model_path))\n",
        "\n",
        "if cuda:\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n",
        "    net = net.cuda()\n",
        "\n",
        "\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum,\n",
        "                      weight_decay=weight_decay)\n",
        "optimizer.load_state_dict(torch.load(optim_path))\n",
        "\n",
        "criterion = MultiBoxLoss(cfg['num_classes'], 0.5, True, 0, True, 3, 0.5,\n",
        "                         False, cuda)\n",
        "\n",
        "net.train()\n",
        "# loss counters\n",
        "loc_loss = 0\n",
        "conf_loss = 0\n",
        "epoch = 0\n",
        "\n",
        "train_iterator = iter(train_loader)\n",
        "\n",
        "\n",
        "message_s=\"\"\n",
        "\n",
        "  for epoch in range(start_epoch+1, epochs):\n",
        "      \n",
        "\n",
        "        loc_loss = 0\n",
        "        conf_loss = 0\n",
        "      \n",
        "    \n",
        "        net.train(True)  # Set model to training mode\n",
        "        try:\n",
        "          images,targets = next(train_iterator)\n",
        "        except StopIteration:\n",
        "          train_iterator = iter(train_loader)\n",
        "          images,targets = next(train_iterator)\n",
        "      \n",
        "            \n",
        "        images = Variable(images.cuda())\n",
        "        targets = [Variable(ann.cuda(), requires_grad=True) for ann in targets]\n",
        "        \n",
        "       \n",
        "        # forward\n",
        "        t0 = time.time()\n",
        "        out = net(images)\n",
        "        # backprop\n",
        "        optimizer.zero_grad()\n",
        "        loss_l, loss_c = criterion(out, targets)\n",
        "        loss = loss_l + loss_c\n",
        "        \n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "          \n",
        "        t1 = time.time()\n",
        "        loc_loss += loss_l.item()\n",
        "        conf_loss += loss_c.item()\n",
        "\n",
        "        \n",
        "        message_s +=\"epoch:\"+str(epoch)+' timer: ' +str(t1 - t0)\\\n",
        "        + \" loss: \"+str(loss.item())+ \" loc_loss: \"+str(loc_loss) +\" conf_loss: \"+str(conf_loss)+\"\\n\"\n",
        "\n",
        "        if epoch %1000==0:\n",
        "          print(\" epoch: \"+str(epoch)+\" loss:\"+ str(loss))\n",
        "\n",
        "        #by 4000 step save traned model\n",
        "        if epoch % 4000 == 0:\n",
        "              print(\"saveing...\")\n",
        "              torch.save(ssd_net.state_dict(),  f\"/content/drive/My Drive/checkpoints/ssd_ckpt_%d.pth\" % epoch)\n",
        "              torch.save(optimizer.state_dict(),  f\"/content/drive/My Drive/checkpoints/optim_ckpt_%d.pth\" % epoch)\n",
        "              #save loss log\n",
        "              the_file = open(\"/content/drive/My Drive/ssdlog_message_%d.txt\"% (epoch),'w')\n",
        "              the_file.write(message_s)\n",
        "              the_file.close()\n",
        "              message_s =\"\"\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxvPBQbjW_OG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#eval script\n",
        "from ssd import *\n",
        "import os\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn.init as init\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "\n",
        "\n",
        "imgs_path = './Insight-MVT_Annotation_Train/'\n",
        "labels_path = '/content/drive/My Drive/ssd_val.csv'\n",
        "model_path = \"/content/drive/My Drive/checkpoints/ssd_ckpt_%d.pth\"% 57000\n",
        "\n",
        "\n",
        "net = build_ssd('test', 300, num_classes=2)            # initialize SSD\n",
        "net.load_state_dict(torch.load(model_path))\n",
        "net.eval()\n",
        "\n",
        "dataset = Transform(imgs_path,labels_path,transform=SSDAugmentation())\n",
        "\n",
        "train_loader = DataLoader(dataset,batch_size=1,num_workers=0)\n",
        "\n",
        "\n",
        "iou_threshold=0.5\n",
        "w = 1280\n",
        "h = 720\n",
        "all_detections = []\n",
        "all_annotations = []\n",
        "m=0\n",
        "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "\n",
        "for data_ in train_loader:\n",
        "   \n",
        "    im = data_[0][0]\n",
        "    gt= data_[1][0]\n",
        "    gt[:,0] *= w\n",
        "    gt[:,2] *= w\n",
        "    gt[:,1] *= h\n",
        "    gt[:,3] *= h\n",
        "    all_annotations.append(gt)\n",
        "\n",
        "    x = Variable(im.unsqueeze(0))\n",
        "    x = x.cuda()\n",
        "    x = x.float()\n",
        "    detections = net(x)[0][1] #just veehicle class\n",
        "\n",
        "    \n",
        "    mask = detections[:, 0].gt(0.).expand(5, detections.size(0)).t()\n",
        "    detections = torch.masked_select(detections, mask).view(-1, 5)\n",
        "    detections[:,1] *= w\n",
        "    detections[:,3] *= w\n",
        "    detections[:,2] *= h\n",
        "    detections[:,4] *= h\n",
        "    all_detections.append(detections)\n",
        "    \n",
        "\n",
        "def _compute_ap(recall, precision):\n",
        "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
        "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
        "    # Arguments\n",
        "        recall:    The recall curve (list).\n",
        "        precision: The precision curve (list).\n",
        "    # Returns\n",
        "        The average precision as computed in py-faster-rcnn.\n",
        "    \"\"\"\n",
        "    # correct AP calculation\n",
        "    # first append sentinel values at the end\n",
        "    mrec = np.concatenate(([0.], recall, [1.]))\n",
        "    mpre = np.concatenate(([0.], precision, [0.]))\n",
        "\n",
        "    # compute the precision envelope\n",
        "    for i in range(mpre.size - 1, 0, -1):\n",
        "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
        "\n",
        "    # to calculate area under PR curve, look for points\n",
        "    # where X axis (recall) changes value\n",
        "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
        "\n",
        "    # and sum (\\Delta recall) * prec\n",
        "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
        "    return ap\n",
        "\n",
        "\n",
        "false_positives = np.zeros((0,))\n",
        "true_positives  = np.zeros((0,))\n",
        "scores          = np.zeros((0,))\n",
        "num_annotations = 0.0\n",
        "print(\"dataset len: \",len(dataset))\n",
        "for i in range(len(dataset)):\n",
        "    detections = all_detections[i]\n",
        "    annotations = all_annotations[i]\n",
        "    num_annotations  += annotations.shape[0]\n",
        "    detected_annotations = []\n",
        " \n",
        "   \n",
        "    for d in detections:\n",
        "        scores = np.append(scores, d[0].cpu().detach())\n",
        "\n",
        "        if annotations.shape[0] == 0:\n",
        "            false_positives = np.append(false_positives, 1)\n",
        "            true_positives  = np.append(true_positives, 0)\n",
        "            continue\n",
        "        d = d.unsqueeze(0)\n",
        "        overlaps = jaccard(d[:,1:].float(), annotations[:,:4].float())\n",
        "        overlaps = overl+aps.cpu().detach().numpy()\n",
        "        assigned_annotation = np.argmax(overlaps, axis=1)\n",
        "        max_overlap         = overlaps[0, assigned_annotation]\n",
        "        \n",
        "        if max_overlap >= iou_threshold and assigned_annotation not in detected_annotations:\n",
        "          false_positives = np.append(false_positives, 0)\n",
        "          true_positives  = np.append(true_positives, 1)\n",
        "          detected_annotations.append(assigned_annotation)\n",
        "        else:\n",
        "          false_positives = np.append(false_positives, 1)\n",
        "          true_positives  = np.append(true_positives, 0)\n",
        "\n",
        "if num_annotations == 0:\n",
        "  average_precisions[label] = 0, 0\n",
        "\n",
        "# sort by score\n",
        "indices = np.argsort(-scores)\n",
        "false_positives = false_positives[indices]\n",
        "true_positives  = true_positives[indices]\n",
        "\n",
        "# compute false positives and true positives\n",
        "false_positives = np.cumsum(false_positives)\n",
        "true_positives  = np.cumsum(true_positives)\n",
        "\n",
        "# compute recall and precision\n",
        "recall    = true_positives / num_annotations\n",
        "precision = true_positives / np.maximum(true_positives + false_positives, np.finfo(np.float64).eps)\n",
        "\n",
        "average_precision  = _compute_ap(recall, precision)\n",
        "r = recall[-1]\n",
        "p = precision[-1]\n",
        "f1 = 2*r*p/(r+p+1e-6)\n",
        "print(\"recall: \",r)\n",
        "print(\"precision: \", p)\n",
        "print(\"average_precision: \",average_precision)\n",
        "print(\"f1_score: \", f1)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-wDvZUXL6FF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#detector script\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "import torch\n",
        "from ssd import *\n",
        "import numpy as np \n",
        "import cv2\n",
        "import time\n",
        "\n",
        "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "def base_transform(image, size, mean):\n",
        "    x = cv2.resize(image, (size, size)).astype(np.float32)\n",
        "    x -= mean\n",
        "    x = x.astype(np.float32)\n",
        "    return x\n",
        "\n",
        "class BaseTransform:\n",
        "    def __init__(self, size, mean):\n",
        "        self.size = size\n",
        "        self.mean = np.array(mean, dtype=np.float32)\n",
        "\n",
        "    def __call__(self, image, boxes=None, labels=None):\n",
        "        return base_transform(image, self.size, self.mean), boxes, labels\n",
        "\n",
        "model_path = \"/content/drive/My Drive/checkpoints/ssd_ckpt_%d.pth\"% 57000\n",
        "net = build_ssd('test', 300, 2)    # initialize SSD\n",
        "net.load_state_dict(torch.load(model_path))\n",
        "\n",
        "COLORS = [(255, 0, 0), (0, 255, 0), (0, 0, 255)]\n",
        "FONT = cv2.FONT_HERSHEY_SIMPLEX\n",
        "transform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\n",
        "\n",
        "\n",
        "#path of test image\n",
        "frame = cv2.imread('./Insight-MVT_Annotation_Train/MVI_20061/img00300.jpg')\n",
        "height, width = frame.shape[:2]\n",
        "x = torch.from_numpy(transform(frame)[0]).permute(2, 0, 1)\n",
        "x = Variable(x.unsqueeze(0))\n",
        "start_time = time.time()\n",
        "y = net(x.cuda())  # forward pass\n",
        "end_time = time.time()\n",
        "\n",
        "print(\"diff time: \",start_time - end_time)\n",
        "detections = y.data\n",
        "# scale each detection back up to the image\n",
        "scale = torch.Tensor([width, height, width, height])\n",
        "for i in range(detections.size(1)):\n",
        "    j = 0\n",
        "    while detections[0, i, j, 0] >= 0.5:\n",
        "        #print(detections[0, i, j, :])\n",
        "        pt = (detections[0, i, j, 1:] * scale).cpu().numpy()\n",
        "        #print(pt)\n",
        "        cv2.rectangle(frame,\n",
        "                      (int(pt[0]), int(pt[1])),\n",
        "                      (int(pt[2]), int(pt[3])),\n",
        "                      COLORS[i % 3], 2)\n",
        "        cv2.putText(frame, \"vehicle\", (int(pt[0]), int(pt[1])),\n",
        "                    FONT, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "        j += 1\n",
        "\n",
        "cv2_imshow(frame)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZlihZFB6Z2x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERrB3wvliNXp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from sort import *\n",
        "########  tracking video ###########################\n",
        "%matplotlib inline\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "\n",
        "from ssd import *\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import cv2\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "VIDEO_SAVE_DIR=\"traffic_result\"\n",
        "if(os.path.exists(VIDEO_SAVE_DIR)):\n",
        "  !rm -r traffic_result\n",
        "  os.mkdir(VIDEO_SAVE_DIR)\n",
        "else:\n",
        "  os.mkdir(VIDEO_SAVE_DIR)\n",
        "\n",
        "def detection(frame):\n",
        "  height, width = frame.shape[:2]\n",
        "  x = torch.from_numpy(transform(frame)[0]).permute(2, 0, 1)\n",
        "  x = Variable(x.unsqueeze(0))\n",
        "  x = x.cuda()\n",
        "  y = net(x)  # forward pass\n",
        "  detections = y.data\n",
        "  final_detections = []\n",
        "  # scale each detection back up to the image\n",
        "  scale = torch.Tensor([width, height, width, height])\n",
        "  for i in range(detections.size(1)):\n",
        "      j = 0\n",
        "      while detections[0, i, j, 0] >= 0.5:\n",
        "          j += 1\n",
        "          final_detections.append(detections[0, i, j, :])\n",
        "\n",
        "  result = []\n",
        "\n",
        "  for d in final_detections:\n",
        "    result.append(d.cpu().detach().numpy())\n",
        "\n",
        "  result = np.asarray(result)  \n",
        "  if(result.size != 0):\n",
        "    result = np.roll(result,-1,axis=1) \n",
        "  return result\n",
        "\n",
        "def base_transform(image, size, mean):\n",
        "    x = cv2.resize(image, (size, size)).astype(np.float32)\n",
        "    x -= mean\n",
        "    x = x.astype(np.float32)\n",
        "    return x\n",
        "\n",
        "class BaseTransform:\n",
        "    def __init__(self, size, mean):\n",
        "        self.size = size\n",
        "        self.mean = np.array(mean, dtype=np.float32)\n",
        "\n",
        "    def __call__(self, image, boxes=None, labels=None):\n",
        "        return base_transform(image, self.size, self.mean), boxes, labels\n",
        "\n",
        "\n",
        "COLORS = [(255, 0, 0), (0, 255, 0), (0, 0, 255)]\n",
        "FONT = cv2.FONT_HERSHEY_SIMPLEX\n",
        "start_epoch=57000\n",
        "model_path = \"/content/drive/My Drive/checkpoints/ssd_ckpt_%d.pth\"% start_epoch\n",
        "net = build_ssd('test', 300, 2)    # initialize SSD\n",
        "net.load_state_dict(torch.load(model_path))\n",
        "transform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\n",
        "\n",
        "#capture = cv2.VideoCapture(\"/content/drive/My Drive/3.mp4\")\n",
        "\n",
        "frames = []\n",
        "frame_count=0\n",
        "mot_tracker = Sort()\n",
        "colours = np.random.rand(32,3)*255\n",
        "\n",
        "frame_path = []\n",
        "for name in os.listdir(\"/content/Insight-MVT_Annotation_Train/MVI_40131\"):\n",
        "  frame_path.append(\"/content/Insight-MVT_Annotation_Train/MVI_40131/\"+name)\n",
        "\n",
        "frame_path = sorted(frame_path)\n",
        "for path in frame_path:\n",
        "    #ret, frame = capture.read()\n",
        "    # if not ret:\n",
        "    #     break  \n",
        "    frame = cv2.imread(path) \n",
        "    frame_count += 1\n",
        "\n",
        "    if(frame_count % 2== 0):\n",
        "      \n",
        "\n",
        "      height, width = frame.shape[:2]\n",
        "      detections = detection(frame)\n",
        "      scale = np.array([width,height,width,height])\n",
        "      \n",
        "      if detections.size != 0:\n",
        "        detections = (detections[:,:4]* scale)\n",
        "        #trackers_objects  = mot_tracker.update(detections)\n",
        "        trackers = mot_tracker.update(detections)\n",
        "        \n",
        "        for d in trackers:\n",
        "          try:\n",
        "              xmin=int(d[2])\n",
        "              ymin=int(d[3])\n",
        "              xmax=int(d[0])\n",
        "              ymax=int(d[1])\n",
        "              label=int(d[4])\n",
        "              cv2.rectangle(frame,(xmin,ymin),(xmax,ymax),(255,0,0),2)\n",
        "              cv2.putText(frame, str(label), (int(xmin), int(ymin)),\n",
        "                      cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 4, cv2.LINE_AA)\n",
        "          except:\n",
        "            print(\"error\")            \n",
        "      \n",
        "        name = '{}.jpg'.format(frame_count)\n",
        "        name = os.path.join(VIDEO_SAVE_DIR, name)\n",
        "        cv2.imwrite(name, frame)\n",
        "    if(frame_count % 100==0):\n",
        "      print(frame_count)\n",
        "      \n",
        "\n",
        "#convert tracked image to video\n",
        "\n",
        "#-----------------CONVERT IMAGE TO VIDEO--------------------\n",
        "import glob\n",
        "import os\n",
        "import cv2\n",
        "# video = cv2.VideoCapture(\"/content/drive/My Drive/3.mp4\");\n",
        "\n",
        "# # Find OpenCV version\n",
        "# (major_ver, minor_ver, subminor_ver) = (cv2.__version__).split('.')\n",
        "\n",
        "# if int(major_ver)  < 3 :\n",
        "#     fps = video.get(cv2.cv.CV_CAP_PROP_FPS)\n",
        "#     print(\"Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): {0}\".format(fps))\n",
        "# else :\n",
        "#     fps = video.get(cv2.CAP_PROP_FPS)\n",
        "#     print(\"Frames per second using video.get(cv2.CAP_PROP_FPS) : {0}\".format(fps))\n",
        "\n",
        "#video.release();\n",
        "fps=25\n",
        "def make_video(outvid, images=None, fps=30, size=None,\n",
        "               is_color=True, format=\"FMP4\"):\n",
        "  \n",
        "    from cv2 import VideoWriter, VideoWriter_fourcc, imread, resize\n",
        "    fourcc = VideoWriter_fourcc(*\"FMP4\")\n",
        "    vid = None\n",
        "    for image in images:\n",
        "        #print(image)\n",
        "        if not os.path.exists(image):\n",
        "            raise FileNotFoundError(image)\n",
        "        img = imread(image)\n",
        "        if vid is None:\n",
        "            if size is None:\n",
        "                size = img.shape[1], img.shape[0]\n",
        "            vid = VideoWriter(outvid, fourcc, float(fps), size, is_color)\n",
        "        if size[0] != img.shape[1] and size[1] != img.shape[0]:\n",
        "            img = resize(img, size)\n",
        "        vid.write(img)\n",
        "    vid.release()\n",
        "    return vid\n",
        "\n",
        "\n",
        "\n",
        "images = list(glob.iglob(os.path.join(\"traffic_result\", '*.*')))\n",
        "# Sort the images by name index.\n",
        "images = sorted(images, key=lambda x: float(os.path.split(x)[1][:-3]))  \n",
        "\n",
        "outvid =  \"/content/drive/My Drive/ssd_out.mp4\"\n",
        "make_video(outvid, images, fps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufqXzpxZTkbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/img_tracking2.zip\", 'r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-I6ts10vvln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### calculate accuracy for tracking ---------------\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "\n",
        "from ssd import *\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import cv2\n",
        "import os\n",
        "import warnings\n",
        "from sort import *\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "VIDEO_SAVE_DIR=\"traffic_result\"\n",
        "if(os.path.exists(VIDEO_SAVE_DIR)):\n",
        "  !rm -r traffic_result\n",
        "  os.mkdir(VIDEO_SAVE_DIR)\n",
        "else:\n",
        "  os.mkdir(VIDEO_SAVE_DIR)\n",
        "\n",
        "def detection(frame):\n",
        "  height, width = frame.shape[:2]\n",
        "  x = torch.from_numpy(transform(frame)[0]).permute(2, 0, 1)\n",
        "  x = Variable(x.unsqueeze(0))\n",
        "  x = x.cuda()\n",
        "  y = net(x)  # forward pass\n",
        "  detections = y.data\n",
        "  final_detections = []\n",
        "  # scale each detection back up to the image\n",
        "  scale = torch.Tensor([width, height, width, height])\n",
        "  for i in range(detections.size(1)):\n",
        "      j = 0\n",
        "      while detections[0, i, j, 0] >= 0.5:\n",
        "          j += 1\n",
        "          final_detections.append(detections[0, i, j, :])\n",
        "\n",
        "  result = []\n",
        "\n",
        "  for d in final_detections:\n",
        "    result.append(d.cpu().detach().numpy())\n",
        "\n",
        "  result = np.asarray(result)  \n",
        "  if(result.size != 0):\n",
        "    result = np.roll(result,-1,axis=1) \n",
        "  return result\n",
        "\n",
        "def base_transform(image, size, mean):\n",
        "    x = cv2.resize(image, (size, size)).astype(np.float32)\n",
        "    x -= mean\n",
        "    x = x.astype(np.float32)\n",
        "    return x\n",
        "\n",
        "class BaseTransform:\n",
        "    def __init__(self, size, mean):\n",
        "        self.size = size\n",
        "        self.mean = np.array(mean, dtype=np.float32)\n",
        "\n",
        "    def __call__(self, image, boxes=None, labels=None):\n",
        "        return base_transform(image, self.size, self.mean), boxes, labels\n",
        "\n",
        "\n",
        "COLORS = [(255, 0, 0), (0, 255, 0), (0, 0, 255)]\n",
        "FONT = cv2.FONT_HERSHEY_SIMPLEX\n",
        "start_epoch=57000\n",
        "model_path = \"/content/drive/My Drive/checkpoints/ssd_ckpt_%d.pth\"% start_epoch\n",
        "net = build_ssd('test', 300, 2)    # initialize SSD\n",
        "net.load_state_dict(torch.load(model_path))\n",
        "transform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "frames = []\n",
        "frame_count=0\n",
        "mot_tracker = Sort()\n",
        "colours = np.random.rand(32,3)*255\n",
        "\n",
        "the_file = open('test_tracking.txt')\n",
        "list_of_test_img = []\n",
        "\n",
        "for line in the_file:\n",
        "  list_of_test_img.append(line.strip())\n",
        "\n",
        "\n",
        "list_of_test_img = sorted(list_of_test_img)\n",
        "\n",
        "all_ = []\n",
        "path = \"/content/Insight-MVT_Annotation_Train/\"\n",
        "\n",
        "\n",
        "for id,filename in enumerate(list_of_test_img):\n",
        "\n",
        "    frame = cv2.imread(path+\"/\"+str(filename),1)\n",
        "    #print(frame.shape)\n",
        "    # Save each frame of the video to a list\n",
        "    frame_count += 1\n",
        "    height, width = frame.shape[:2]\n",
        "    #print(height, width )\n",
        "    detections = detection(frame)\n",
        "    scale = np.array([width,height,width,height])\n",
        "    if frame_count % 100 == 0:\n",
        "      print(frame_count)\n",
        "\n",
        "    if detections.size != 0:\n",
        "      \n",
        "      detections = (detections[:,:4]* scale)\n",
        "      #trackers_objects  = mot_tracker.update(detections)\n",
        "      trackers = mot_tracker.update(detections)\n",
        "      #print(trackers)\n",
        "      for d in trackers:\n",
        "        try:\n",
        "            xmin=int(d[0])\n",
        "            ymin=int(d[1])\n",
        "            xmax=int(d[2])\n",
        "            ymax=int(d[3])\n",
        "            label=int(d[4])\n",
        "            cv2.rectangle(frame,(xmin,ymin),(xmax,ymax),(255,0,0),2)\n",
        "            cv2.putText(frame, str(label), (int(xmin), int(ymin)),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 4, cv2.LINE_AA)\n",
        "            \n",
        "\n",
        "            all_.append([filename,label,xmin,ymin,xmax,ymax])\n",
        "        except:\n",
        "          print(\"error\")            \n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(all_, columns=[\"name\",\"label\",\"x1\",\"y1\",'x2','y2'])\n",
        "df.to_csv('ssd_track_out.csv', index=False) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFASirTKYF4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFRIuZmN7_Ui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}